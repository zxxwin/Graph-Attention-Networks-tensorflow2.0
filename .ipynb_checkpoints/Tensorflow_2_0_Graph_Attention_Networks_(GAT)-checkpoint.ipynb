{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, print_function, division, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "control_flow_util.ENABLE_CONTROL_FLOW_V2 = True\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = 'cora'\n",
    "Sparse = False \n",
    "Batch_Size = 1 \n",
    "Epochs = 100000\n",
    "Patience = 100\n",
    "Learning_Rate = 0.005\n",
    "Weight_Decay = 0.0005\n",
    "ffd_drop = 0.6\n",
    "attn_drop = 0.6\n",
    "Residual = False\n",
    "\n",
    "dataset = Dataset\n",
    "\n",
    "# training params\n",
    "batch_size = Batch_Size\n",
    "nb_epochs = Epochs\n",
    "patience = Patience\n",
    "lr = Learning_Rate\n",
    "l2_coef = Weight_Decay\n",
    "residual = Residual\n",
    "\n",
    "\n",
    "hid_units = [8] # numbers of hidden units per each attention head in each layer\n",
    "n_heads = [8, 1] # additional entry for the output layer\n",
    "\n",
    "nonlinearity = tf.nn.elu\n",
    "optimizer = tf.keras.optimizers.Adam(lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    " Prepare adjacency matrix by expanding up to a given neighbourhood.\n",
    " This will insert loops on every node.\n",
    " Finally, the matrix is converted to bias vectors.\n",
    " Expected shape: [graph, nodes, nodes]\n",
    "\"\"\"\n",
    "def adj_to_bias(adj, sizes, nhood=1):\n",
    "    nb_graphs = adj.shape[0]\n",
    "    mt = np.empty(adj.shape)\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
    "        for i in range(sizes[g]):\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "# This section of code adapted from tkipf/gcn #\n",
    "###############################################\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']  #ind.cora.test.index left out\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))                   \n",
    "      \n",
    "    \n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    # print(f'This is graph {graph},{len(graph)}')\n",
    "    \n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    \n",
    "    test_idx_range = np.sort(test_idx_reorder) #test_idx_range =  [1708,1709,...,2707] 1000 entries\n",
    "       \n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil() \n",
    "    \n",
    "    \n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    print(f'adj.shape: {adj.shape}')\n",
    "    print(f'features.shape: {features.shape}')\n",
    "\n",
    "\n",
    "    \n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_random_data(size):\n",
    "\n",
    "    adj = sp.random(size, size, density=0.002) # density similar to cora\n",
    "    features = sp.random(size, 1000, density=0.015)\n",
    "    int_labels = np.random.randint(7, size=(size))\n",
    "    labels = np.zeros((size, 7)) # Nx7\n",
    "    labels[np.arange(size), int_labels] = 1\n",
    "\n",
    "    train_mask = np.zeros((size,)).astype(bool)\n",
    "    train_mask[np.arange(size)[0:int(size/2)]] = 1\n",
    "\n",
    "    val_mask = np.zeros((size,)).astype(bool)\n",
    "    val_mask[np.arange(size)[int(size/2):]] = 1\n",
    "\n",
    "    test_mask = np.zeros((size,)).astype(bool)\n",
    "    test_mask[np.arange(size)[int(size/2):]] = 1\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "  \n",
    "    # sparse NxN, sparse NxF, norm NxC, ..., norm Nx1, ...\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def standardize_data(f, train_mask):\n",
    "    \"\"\"Standardize feature matrix and convert to tuple representation\"\"\"\n",
    "    # standardize data\n",
    "    f = f.todense()\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = f[:, np.squeeze(np.array(sigma > 0))]\n",
    "    mu = f[train_mask == True, :].mean(axis=0)\n",
    "    sigma = f[train_mask == True, :].std(axis=0)\n",
    "    f = (f - mu) / sigma\n",
    "    return f\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "def preprocess_adj_bias(adj):\n",
    "    num_nodes = adj.shape[0]\n",
    "    adj = adj + sp.eye(num_nodes)  # self-loop\n",
    "    adj[adj > 0.0] = 1.0\n",
    "    if not sp.isspmatrix_coo(adj):\n",
    "        adj = adj.tocoo()\n",
    "    adj = adj.astype(np.float32)\n",
    "    # This is where I made a mistake, I used (adj.row, adj.col) instead \n",
    "    indices = np.vstack((adj.col, adj.row)).transpose()    \n",
    "    \n",
    "    return tf.SparseTensor(indices=indices, values=adj.data, dense_shape=adj.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class attn_head(tf.keras.layers.Layer):\n",
    "    def __init__(self,hidden_dim, nb_nodes = None,in_drop=0.0, coef_drop=0.0,activation = tf.nn.elu,residual = False):        \n",
    "        super(attn_head,self).__init__()        \n",
    "        self.activation = activation\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.in_dropout = tf.keras.layers.Dropout(in_drop)\n",
    "        self.coef_dropout = tf.keras.layers.Dropout(coef_drop)        \n",
    "        self.conv_no_bias = tf.keras.layers.Conv1D(hidden_dim,1,use_bias=False)\n",
    "        self.conv_f1 = tf.keras.layers.Conv1D(1,1)\n",
    "        self.conv_f2 = tf.keras.layers.Conv1D(1,1)\n",
    "                \n",
    "        self.conv_residual = tf.keras.layers.Conv1D(hidden_dim,1)\n",
    "        self.bias_zero = tf.Variable(tf.zeros(hidden_dim))\n",
    "        \n",
    "    def __call__(self,seq,bias_mat,training):\n",
    "                \n",
    "        # 输入的节点特征\n",
    "        seq = self.in_dropout(seq,training = training)\n",
    "        # 使用 hidden_dim=8 个1维卷积，卷积核大小为1\n",
    "        # 相当于 Wh\n",
    "        # seq_fts.shape: (num_graph, num_nodes, hidden_dim)\n",
    "        seq_fts = self.conv_no_bias(seq)\n",
    "        # 1x1 卷积可以理解为按hidden_dim这个通道进行加权求和，但参数共享\n",
    "        # 相当于单输出全连接层1\n",
    "        # f_1.shape: (num_graph, num_nodes, 1)\n",
    "        f_1 = self.conv_f1(seq_fts)\n",
    "        # 相当于单输出全连接层2\n",
    "        f_2 = self.conv_f2(seq_fts)\n",
    "        \n",
    "        # 广播机制计算(num_graph,num_nodes,1)+(num_graph,1,num_nodes)\n",
    "        # logits.shape: (num_graph, num_nodes, num_nodes)\n",
    "        # 相当于计算了所有节点的 [e_ij]\n",
    "        logits = f_1 + tf.transpose(f_2,[0,2,1])\n",
    "        # 得到邻居节点的注意力系数：[alpha_ij]\n",
    "        # coefs.shape: (num_graph, num_nodes, num_nodes)\n",
    "        coefs = tf.nn.softmax(tf.nn.leaky_relu(logits)+bias_mat)\n",
    "        # dropout\n",
    "        coefs = self.coef_dropout(coefs,training = training)\n",
    "        seq_fts = self.in_dropout(seq_fts,training = training)\n",
    "        # 计算：[alpha_ij] x Wh\n",
    "        # vals.shape: (num_graph, num_nodes, num_nodes)\n",
    "        vals = tf.matmul(coefs, seq_fts)\n",
    "        vals = tf.cast(vals, dtype=tf.float32)\n",
    "        # 最终结果再加上一个 bias \n",
    "        ret = vals + self.bias_zero\n",
    "        # 残差\n",
    "        if self.residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + self.conv_residual(seq)                \n",
    "            else:\n",
    "                ret = ret + seq\n",
    "        # 返回 h' = σ([alpha_ij] x Wh)\n",
    "        # shape: (num_graph, num_nodes, hidden_dim)\n",
    "        return self.activation(ret)\n",
    "    \n",
    "    \n",
    "class sp_attn_head(tf.keras.layers.Layer):\n",
    "    def __init__(self,hidden_dim, nb_nodes,in_drop=0.0, coef_drop=0.0,activation = tf.nn.elu,residual = False):        \n",
    "        super(sp_attn_head,self).__init__()     \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nb_nodes = nb_nodes\n",
    "        self.activation = activation\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.in_dropout = tf.keras.layers.Dropout(in_drop)\n",
    "        self.coef_dropout = tf.keras.layers.Dropout(coef_drop)        \n",
    "        \n",
    "        self.conv_no_bias = tf.keras.layers.Conv1D(hidden_dim,1,use_bias=False)\n",
    "        self.conv_f1 = tf.keras.layers.Conv1D(1,1)\n",
    "        self.conv_f2 = tf.keras.layers.Conv1D(1,1)\n",
    "                \n",
    "        self.conv_residual = tf.keras.layers.Conv1D(hidden_dim,1)\n",
    "        self.bias_zero = tf.Variable(tf.zeros(hidden_dim))\n",
    "        \n",
    "    def __call__(self,seq,bias_mat,training):\n",
    "\n",
    "        adj_mat = bias_mat\n",
    "        seq = self.in_dropout(seq,training = training)\n",
    "        seq_fts = self.conv_no_bias(seq)\n",
    "        f_1 = self.conv_f1(seq_fts)\n",
    "        f_2 = self.conv_f2(seq_fts)\n",
    "        \n",
    "        f_1 = tf.reshape(f_1, (self.nb_nodes, 1))\n",
    "        f_1 = adj_mat*f_1\n",
    "        f_2 = tf.reshape(f_2, (self.nb_nodes, 1))\n",
    "        f_2 = adj_mat * tf.transpose(f_2, [1,0])\n",
    "        logits = tf.compat.v1.sparse_add(f_1,f_2)\n",
    "\n",
    "\n",
    "        lrelu = tf.SparseTensor(indices=logits.indices, \n",
    "                        values=tf.nn.leaky_relu(logits.values), \n",
    "                        dense_shape=logits.dense_shape)\n",
    "        coefs = tf.compat.v2.sparse.softmax(lrelu)\n",
    "        \n",
    "        if training != False:\n",
    "            coefs = tf.SparseTensor(indices=coefs.indices,\n",
    "                                    values=self.coef_dropout(coefs.values,training = training),\n",
    "                                    dense_shape=coefs.dense_shape)\n",
    "            seq_fts = self.in_dropout(seq_fts,training = training)\n",
    "        \n",
    "        coefs = tf.compat.v2.sparse.reshape(coefs, [nb_nodes, nb_nodes])\n",
    "        \n",
    "        seq_fts = tf.squeeze(seq_fts)\n",
    "        vals = tf.sparse.sparse_dense_matmul(coefs, seq_fts)\n",
    "        vals = tf.expand_dims(vals, axis=0)\n",
    "        vals.set_shape([1, self.nb_nodes, self.hidden_dim])\n",
    "        \n",
    "        ret = vals + self.bias_zero\n",
    "        if self.residual:\n",
    "            if seq.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + self.conv_residual(seq)                \n",
    "            else:\n",
    "                ret = ret + seq\n",
    "        return self.activation(ret)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_attn_head(Sparse):\n",
    "    if Sparse:\n",
    "        chosen_attention = sp_attn_head\n",
    "    else:\n",
    "        chosen_attention = attn_head\n",
    "    \n",
    "    return chosen_attention\n",
    "\n",
    "class inference(tf.keras.layers.Layer):\n",
    "    def __init__(self,n_heads,hid_units,nb_classes, nb_nodes,Sparse,ffd_drop=0.0, attn_drop=0.0,activation = tf.nn.elu,residual = False):        \n",
    "        super(inference,self).__init__()\n",
    "        attned_head = choose_attn_head(Sparse)\n",
    "        self.attns = []\n",
    "        self.sec_attns = []\n",
    "        self.final_attns = []\n",
    "        self.final_sum = n_heads[-1]\n",
    "        # 构造 n_heads[0] 个 attention\n",
    "        for i in range(n_heads[0]):\n",
    "            self.attns.append(attned_head(hidden_dim = hid_units[0], nb_nodes = nb_nodes,\n",
    "                                            in_drop = ffd_drop, coef_drop = attn_drop, \n",
    "                                            activation = activation,\n",
    "                                            residual = residual))\n",
    "        \n",
    "        # hid_units表示每一个attention head中每一层的隐藏单元个数\n",
    "        # 若给定hid_units = [8], 表示使用单个全连接层\n",
    "        # 因此，不执行下面的代码\n",
    "        for i in range(1, len(hid_units)):\n",
    "            h_old = h_1\n",
    "            sec_attns = []\n",
    "            for j in range(n_heads[i]):                \n",
    "                sec_attns.append(attned_head(hidden_dim = hid_units[i], nb_nodes = nb_nodes,\n",
    "                                             in_drop = ffd_drop, coef_drop = attn_drop, \n",
    "                                             activation = activation,\n",
    "                                             residual = residual))\n",
    "                self.sec_attns.append(sec_attns)\n",
    "                \n",
    "        # 加上输出层\n",
    "        for i in range(n_heads[-1]):\n",
    "            self.final_attns.append(attned_head(hidden_dim = nb_classes, nb_nodes = nb_nodes,                                                                                                         \n",
    "                                                in_drop = ffd_drop, coef_drop = attn_drop, \n",
    "                                                activation = lambda x: x,\n",
    "                                                residual = residual))                \n",
    "\n",
    "    def __call__(self,inputs,bias_mat,training):        \n",
    "        first_attn = []\n",
    "        out = []\n",
    "        # 计算 n_heads[0] 个 attention\n",
    "        for indiv_attn in self.attns:\n",
    "            first_attn.append(indiv_attn(seq = inputs, bias_mat = bias_mat,training = training))\n",
    "        # h_1.shape: (num_graph, num_nodes, hidden_dim*n_heads[0])\n",
    "        h_1 = tf.concat(first_attn,axis = -1)     \n",
    "        # 如果 attention 使用了多层网络，则依次计算\n",
    "        for sec_attns in self.sec_attns:\n",
    "            next_attn = []\n",
    "            for indiv_attns in sec_attns:\n",
    "                next_attn.append(indiv_attn(seq = h_1,bias_mat = bias_mat,training = training))\n",
    "            h_1 = tf.concat(next_attns,axis = -1)\n",
    "        # 得到最终的预测结果\n",
    "        for indiv_attn in self.final_attns:\n",
    "            out.append(indiv_attn(seq=h_1,bias_mat = bias_mat,training = training))\n",
    "        # 将结果在最后一个维度取均值\n",
    "        # logits.shape: (num_graph, num_nodes, nb_classes)\n",
    "        logits = tf.add_n(out)/self.final_sum\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(tf.keras.Model):\n",
    "    def __init__(self, hid_units,n_heads, nb_classes, nb_nodes,Sparse,ffd_drop = 0.0,attn_drop = 0.0,activation = tf.nn.elu,residual=False):    \n",
    "        super(GAT,self).__init__()\n",
    "        '''\n",
    "        hid_units: 隐藏单元个数\n",
    "        n_heads: 每层使用的注意力头个数\n",
    "        nb_classes: 类别数，7\n",
    "        nb_nodes: 节点的个数，2708\n",
    "        activation: 激活函数\n",
    "        residual: 是否使用残差连接\n",
    "        '''                        \n",
    "        self.hid_units = hid_units         #[8]\n",
    "        self.n_heads = n_heads             #[8,1]\n",
    "        self.nb_classes = nb_classes\n",
    "        self.nb_nodes = nb_nodes\n",
    "        self.activation = activation\n",
    "        self.residual = residual        \n",
    "        \n",
    "        self.inferencing = inference(n_heads,hid_units,nb_classes,nb_nodes,Sparse = Sparse,ffd_drop = ffd_drop,attn_drop = attn_drop, activation = activation,residual = residual)\n",
    "        \n",
    "    \n",
    "\n",
    "    def masked_softmax_cross_entropy(self,logits, labels, mask):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        loss *= mask\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def masked_accuracy(self,logits, labels, mask):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        mask /= tf.reduce_mean(mask)\n",
    "        accuracy_all *= mask\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "\n",
    "    \n",
    "    def __call__(self,inputs,training,bias_mat,lbl_in,msk_in):     \n",
    "        # logits.shape: (num_graph, num_nodes, nb_classes)         \n",
    "        logits = self.inferencing(inputs = inputs, bias_mat = bias_mat,training = training)        \n",
    "        \n",
    "        log_resh = tf.reshape(logits, [-1, self.nb_classes])        \n",
    "        lab_resh = tf.reshape(lbl_in, [-1, self.nb_classes])\n",
    "        msk_resh = tf.reshape(msk_in, [-1])        \n",
    "        \n",
    "        loss = self.masked_softmax_cross_entropy(log_resh, lab_resh, msk_resh)\n",
    "        \n",
    "        lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in self.trainable_variables if v.name not\n",
    "                                                     in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "                \n",
    "        loss = loss+lossL2\n",
    "        accuracy = self.masked_accuracy(log_resh, lab_resh, msk_resh)\n",
    "        \n",
    "        return logits,accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,inputs,bias_mat,lbl_in,msk_in,training):        \n",
    "    with tf.GradientTape() as tape:                \n",
    "        logits,accuracy,loss = model(inputs = inputs,\n",
    "                                     training =True,\n",
    "                                     bias_mat = bias_mat,\n",
    "                                     lbl_in =  lbl_in,\n",
    "                                     msk_in =  msk_in)             \n",
    "\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    gradient_variables = zip(gradients, model.trainable_variables)\n",
    "    optimizer.apply_gradients(gradient_variables)        \n",
    "                \n",
    "    return logits,accuracy,loss\n",
    "\n",
    "def evaluate(model,inputs,bias_mat,lbl_in,msk_in,training):                                                        \n",
    "    logits,accuracy,loss = model(inputs= inputs,\n",
    "                                     bias_mat = bias_mat,\n",
    "                                     lbl_in = lbl_in,\n",
    "                                     msk_in = msk_in,\n",
    "                                     training = False)                        \n",
    "    return logits,accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cora\n",
      "----- Opt. hyperparams -----\n",
      "lr: 0.005\n",
      "l2_coef: 0.0005\n",
      "----- Archi. hyperparams -----\n",
      "nb. layers: 1\n",
      "nb. units per layer: [8]\n",
      "nb. attention heads: [8, 1]\n",
      "residual: False\n",
      "nonlinearity: <function elu at 0x000001B39F31CDC8>\n",
      "adj.shape: (2708, 2708)\n",
      "features.shape: (2708, 1433)\n",
      "These are the parameters\n",
      "batch_size: 1\n",
      "nb_nodes: 2708\n",
      "ft_size: 1433\n",
      "nb_classes: 7\n",
      "model: GAT\n",
      "Training: loss = 1.99408, acc = 0.12857 | Val: loss = 1.98312, acc = 0.15800\n",
      "Training: loss = 1.98571, acc = 0.15714 | Val: loss = 1.97586, acc = 0.24000\n",
      "Training: loss = 1.97477, acc = 0.15714 | Val: loss = 1.97048, acc = 0.21600\n",
      "Training: loss = 1.96900, acc = 0.14286 | Val: loss = 1.96593, acc = 0.23600\n",
      "Training: loss = 1.96194, acc = 0.20714 | Val: loss = 1.96145, acc = 0.33600\n",
      "Training: loss = 1.95802, acc = 0.18571 | Val: loss = 1.95750, acc = 0.43800\n",
      "Training: loss = 1.96417, acc = 0.17143 | Val: loss = 1.95373, acc = 0.53200\n",
      "Training: loss = 1.95319, acc = 0.24286 | Val: loss = 1.94983, acc = 0.62400\n",
      "Training: loss = 1.94582, acc = 0.27143 | Val: loss = 1.94655, acc = 0.68400\n",
      "Training: loss = 1.95871, acc = 0.22857 | Val: loss = 1.94332, acc = 0.71800\n",
      "Training: loss = 1.93898, acc = 0.28571 | Val: loss = 1.94101, acc = 0.68800\n",
      "Training: loss = 1.93223, acc = 0.26429 | Val: loss = 1.93996, acc = 0.72600\n",
      "Training: loss = 1.94127, acc = 0.27857 | Val: loss = 1.93963, acc = 0.77200\n",
      "Training: loss = 1.93823, acc = 0.27143 | Val: loss = 1.93902, acc = 0.75000\n",
      "Training: loss = 1.92978, acc = 0.29286 | Val: loss = 1.93871, acc = 0.67800\n",
      "Training: loss = 1.92035, acc = 0.37857 | Val: loss = 1.93760, acc = 0.63800\n",
      "Training: loss = 1.92829, acc = 0.33571 | Val: loss = 1.93579, acc = 0.61600\n",
      "Training: loss = 1.91852, acc = 0.37143 | Val: loss = 1.93482, acc = 0.57400\n",
      "Training: loss = 1.93191, acc = 0.30714 | Val: loss = 1.93270, acc = 0.57800\n",
      "Training: loss = 1.93445, acc = 0.26429 | Val: loss = 1.93103, acc = 0.58000\n",
      "Training: loss = 1.91612, acc = 0.36429 | Val: loss = 1.92946, acc = 0.57000\n",
      "Training: loss = 1.90220, acc = 0.42143 | Val: loss = 1.92764, acc = 0.57600\n",
      "Training: loss = 1.90012, acc = 0.37143 | Val: loss = 1.92542, acc = 0.59600\n",
      "Training: loss = 1.92460, acc = 0.37143 | Val: loss = 1.92254, acc = 0.61600\n",
      "Training: loss = 1.91697, acc = 0.32143 | Val: loss = 1.91969, acc = 0.65000\n",
      "Training: loss = 1.89898, acc = 0.40000 | Val: loss = 1.91662, acc = 0.68000\n",
      "Training: loss = 1.87459, acc = 0.38571 | Val: loss = 1.91340, acc = 0.71400\n",
      "Training: loss = 1.88376, acc = 0.37857 | Val: loss = 1.91055, acc = 0.74000\n",
      "Training: loss = 1.90369, acc = 0.39286 | Val: loss = 1.90730, acc = 0.76000\n",
      "Training: loss = 1.89380, acc = 0.35000 | Val: loss = 1.90431, acc = 0.76400\n",
      "Training: loss = 1.87130, acc = 0.46429 | Val: loss = 1.90169, acc = 0.76800\n",
      "Training: loss = 1.87035, acc = 0.42857 | Val: loss = 1.89882, acc = 0.77000\n",
      "Training: loss = 1.88099, acc = 0.37857 | Val: loss = 1.89603, acc = 0.77200\n",
      "Training: loss = 1.89183, acc = 0.37857 | Val: loss = 1.89400, acc = 0.77800\n",
      "Training: loss = 1.86897, acc = 0.45000 | Val: loss = 1.89202, acc = 0.77600\n",
      "Training: loss = 1.86766, acc = 0.44286 | Val: loss = 1.88978, acc = 0.79200\n",
      "Training: loss = 1.86413, acc = 0.39286 | Val: loss = 1.88759, acc = 0.78800\n",
      "Training: loss = 1.87138, acc = 0.41429 | Val: loss = 1.88518, acc = 0.77600\n",
      "Training: loss = 1.81769, acc = 0.44286 | Val: loss = 1.88291, acc = 0.75800\n",
      "Training: loss = 1.86743, acc = 0.39286 | Val: loss = 1.88135, acc = 0.74200\n",
      "Training: loss = 1.84144, acc = 0.43571 | Val: loss = 1.87979, acc = 0.73400\n",
      "Training: loss = 1.83123, acc = 0.45714 | Val: loss = 1.87727, acc = 0.73400\n",
      "Training: loss = 1.82319, acc = 0.48571 | Val: loss = 1.87401, acc = 0.74400\n",
      "Training: loss = 1.80900, acc = 0.47857 | Val: loss = 1.86999, acc = 0.76600\n",
      "Training: loss = 1.80528, acc = 0.42143 | Val: loss = 1.86573, acc = 0.78200\n",
      "Training: loss = 1.81175, acc = 0.44286 | Val: loss = 1.86146, acc = 0.79000\n",
      "Training: loss = 1.81553, acc = 0.42143 | Val: loss = 1.85774, acc = 0.79400\n",
      "Training: loss = 1.79589, acc = 0.47143 | Val: loss = 1.85345, acc = 0.79200\n",
      "Training: loss = 1.77650, acc = 0.49286 | Val: loss = 1.84962, acc = 0.79400\n",
      "Training: loss = 1.82333, acc = 0.41429 | Val: loss = 1.84614, acc = 0.79000\n",
      "Training: loss = 1.79908, acc = 0.49286 | Val: loss = 1.84215, acc = 0.79000\n",
      "Training: loss = 1.81083, acc = 0.46429 | Val: loss = 1.83873, acc = 0.78400\n",
      "Training: loss = 1.76633, acc = 0.47857 | Val: loss = 1.83591, acc = 0.77400\n",
      "Training: loss = 1.81585, acc = 0.40000 | Val: loss = 1.83134, acc = 0.77200\n",
      "Training: loss = 1.76892, acc = 0.47143 | Val: loss = 1.82695, acc = 0.76800\n",
      "Training: loss = 1.75476, acc = 0.52857 | Val: loss = 1.82325, acc = 0.76400\n",
      "Training: loss = 1.71984, acc = 0.54286 | Val: loss = 1.81928, acc = 0.76400\n",
      "Training: loss = 1.74543, acc = 0.52143 | Val: loss = 1.81576, acc = 0.75800\n",
      "Training: loss = 1.80252, acc = 0.42143 | Val: loss = 1.81249, acc = 0.75400\n",
      "Training: loss = 1.74593, acc = 0.49286 | Val: loss = 1.80917, acc = 0.76000\n",
      "Training: loss = 1.75231, acc = 0.48571 | Val: loss = 1.80651, acc = 0.75400\n",
      "Training: loss = 1.78420, acc = 0.42857 | Val: loss = 1.80267, acc = 0.75000\n",
      "Training: loss = 1.76505, acc = 0.43571 | Val: loss = 1.80044, acc = 0.74600\n",
      "Training: loss = 1.75269, acc = 0.45714 | Val: loss = 1.79791, acc = 0.73800\n",
      "Training: loss = 1.72672, acc = 0.45000 | Val: loss = 1.79525, acc = 0.73600\n",
      "Training: loss = 1.67596, acc = 0.50714 | Val: loss = 1.79280, acc = 0.73600\n",
      "Training: loss = 1.71171, acc = 0.52857 | Val: loss = 1.79041, acc = 0.73800\n",
      "Training: loss = 1.68901, acc = 0.51429 | Val: loss = 1.78668, acc = 0.75400\n",
      "Training: loss = 1.70739, acc = 0.50000 | Val: loss = 1.78275, acc = 0.76600\n",
      "Training: loss = 1.70410, acc = 0.50714 | Val: loss = 1.77762, acc = 0.78200\n",
      "Training: loss = 1.72657, acc = 0.44286 | Val: loss = 1.77200, acc = 0.78800\n",
      "Training: loss = 1.67588, acc = 0.47857 | Val: loss = 1.76524, acc = 0.79800\n",
      "Training: loss = 1.67188, acc = 0.50000 | Val: loss = 1.75760, acc = 0.80400\n",
      "Training: loss = 1.73714, acc = 0.42143 | Val: loss = 1.74962, acc = 0.81000\n",
      "Training: loss = 1.77249, acc = 0.32143 | Val: loss = 1.74273, acc = 0.81400\n",
      "Training: loss = 1.67556, acc = 0.48571 | Val: loss = 1.73623, acc = 0.80800\n",
      "Training: loss = 1.68677, acc = 0.49286 | Val: loss = 1.72954, acc = 0.80400\n",
      "Training: loss = 1.70867, acc = 0.45714 | Val: loss = 1.72387, acc = 0.80400\n",
      "Training: loss = 1.69570, acc = 0.44286 | Val: loss = 1.71853, acc = 0.80400\n",
      "Training: loss = 1.66807, acc = 0.47857 | Val: loss = 1.71392, acc = 0.80400\n",
      "Training: loss = 1.66218, acc = 0.49286 | Val: loss = 1.70953, acc = 0.80000\n",
      "Training: loss = 1.65169, acc = 0.48571 | Val: loss = 1.70517, acc = 0.79800\n",
      "Training: loss = 1.63510, acc = 0.47857 | Val: loss = 1.69930, acc = 0.80400\n",
      "Training: loss = 1.69001, acc = 0.50000 | Val: loss = 1.69510, acc = 0.80600\n",
      "Training: loss = 1.63903, acc = 0.50714 | Val: loss = 1.69152, acc = 0.80400\n",
      "Training: loss = 1.64098, acc = 0.48571 | Val: loss = 1.68856, acc = 0.79600\n",
      "Training: loss = 1.63750, acc = 0.50714 | Val: loss = 1.68497, acc = 0.79400\n",
      "Training: loss = 1.62621, acc = 0.51429 | Val: loss = 1.68116, acc = 0.79600\n",
      "Training: loss = 1.58376, acc = 0.51429 | Val: loss = 1.67725, acc = 0.80000\n",
      "Training: loss = 1.60416, acc = 0.45714 | Val: loss = 1.67484, acc = 0.79800\n",
      "Training: loss = 1.68658, acc = 0.45714 | Val: loss = 1.67114, acc = 0.79800\n",
      "Training: loss = 1.59538, acc = 0.52857 | Val: loss = 1.66752, acc = 0.79800\n",
      "Training: loss = 1.61699, acc = 0.51429 | Val: loss = 1.66461, acc = 0.80200\n",
      "Training: loss = 1.55344, acc = 0.56429 | Val: loss = 1.66171, acc = 0.80400\n",
      "Training: loss = 1.54259, acc = 0.58571 | Val: loss = 1.65852, acc = 0.80400\n",
      "Training: loss = 1.62929, acc = 0.49286 | Val: loss = 1.65637, acc = 0.80400\n",
      "Training: loss = 1.59636, acc = 0.49286 | Val: loss = 1.65441, acc = 0.80000\n",
      "Training: loss = 1.54493, acc = 0.55000 | Val: loss = 1.65199, acc = 0.80200\n",
      "Training: loss = 1.54983, acc = 0.55000 | Val: loss = 1.64970, acc = 0.79800\n",
      "Training: loss = 1.52159, acc = 0.58571 | Val: loss = 1.64633, acc = 0.80000\n",
      "Training: loss = 1.65371, acc = 0.43571 | Val: loss = 1.64145, acc = 0.80000\n",
      "Training: loss = 1.60776, acc = 0.46429 | Val: loss = 1.63719, acc = 0.80400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.64145, acc = 0.52143 | Val: loss = 1.63290, acc = 0.80000\n",
      "Training: loss = 1.61525, acc = 0.48571 | Val: loss = 1.62791, acc = 0.80400\n",
      "Training: loss = 1.57089, acc = 0.47857 | Val: loss = 1.62324, acc = 0.80200\n",
      "Training: loss = 1.53016, acc = 0.51429 | Val: loss = 1.61708, acc = 0.80400\n",
      "Training: loss = 1.72220, acc = 0.45000 | Val: loss = 1.61090, acc = 0.80400\n",
      "Training: loss = 1.56444, acc = 0.55714 | Val: loss = 1.60627, acc = 0.80600\n",
      "Training: loss = 1.53542, acc = 0.50714 | Val: loss = 1.60092, acc = 0.80400\n",
      "Training: loss = 1.60961, acc = 0.45000 | Val: loss = 1.59675, acc = 0.80600\n",
      "Training: loss = 1.56773, acc = 0.50000 | Val: loss = 1.59332, acc = 0.80200\n",
      "Training: loss = 1.57236, acc = 0.50714 | Val: loss = 1.59031, acc = 0.80600\n",
      "Training: loss = 1.60898, acc = 0.54286 | Val: loss = 1.58744, acc = 0.80400\n",
      "Training: loss = 1.53714, acc = 0.52857 | Val: loss = 1.58458, acc = 0.80400\n",
      "Training: loss = 1.50840, acc = 0.51429 | Val: loss = 1.58271, acc = 0.80200\n",
      "Training: loss = 1.53054, acc = 0.52143 | Val: loss = 1.58109, acc = 0.80200\n",
      "Training: loss = 1.59908, acc = 0.54286 | Val: loss = 1.57932, acc = 0.80000\n",
      "Training: loss = 1.58181, acc = 0.50000 | Val: loss = 1.57728, acc = 0.79400\n",
      "Training: loss = 1.48032, acc = 0.62857 | Val: loss = 1.57361, acc = 0.79400\n",
      "Training: loss = 1.49783, acc = 0.55714 | Val: loss = 1.56923, acc = 0.79800\n",
      "Training: loss = 1.50746, acc = 0.52143 | Val: loss = 1.56582, acc = 0.79800\n",
      "Training: loss = 1.60488, acc = 0.50000 | Val: loss = 1.56045, acc = 0.81000\n",
      "Training: loss = 1.51871, acc = 0.53571 | Val: loss = 1.55435, acc = 0.81000\n",
      "Training: loss = 1.55627, acc = 0.52143 | Val: loss = 1.54838, acc = 0.80800\n",
      "Training: loss = 1.55083, acc = 0.52857 | Val: loss = 1.54274, acc = 0.80800\n",
      "Training: loss = 1.47355, acc = 0.56429 | Val: loss = 1.53896, acc = 0.80400\n",
      "Training: loss = 1.51907, acc = 0.52143 | Val: loss = 1.53440, acc = 0.80000\n",
      "Training: loss = 1.57394, acc = 0.50000 | Val: loss = 1.53099, acc = 0.80400\n",
      "Training: loss = 1.52873, acc = 0.47857 | Val: loss = 1.52818, acc = 0.80600\n",
      "Training: loss = 1.57371, acc = 0.45000 | Val: loss = 1.52803, acc = 0.80200\n",
      "Training: loss = 1.70329, acc = 0.36429 | Val: loss = 1.52837, acc = 0.80000\n",
      "Training: loss = 1.53523, acc = 0.50714 | Val: loss = 1.53149, acc = 0.79800\n",
      "Training: loss = 1.60359, acc = 0.52857 | Val: loss = 1.53549, acc = 0.79400\n",
      "Training: loss = 1.44054, acc = 0.55714 | Val: loss = 1.53813, acc = 0.79600\n",
      "Training: loss = 1.48593, acc = 0.53571 | Val: loss = 1.54061, acc = 0.80200\n",
      "Training: loss = 1.46801, acc = 0.52143 | Val: loss = 1.54318, acc = 0.80200\n",
      "Training: loss = 1.53765, acc = 0.53571 | Val: loss = 1.54658, acc = 0.80000\n",
      "Training: loss = 1.56861, acc = 0.52143 | Val: loss = 1.54830, acc = 0.79600\n",
      "Training: loss = 1.56110, acc = 0.50000 | Val: loss = 1.54985, acc = 0.78600\n",
      "Training: loss = 1.59265, acc = 0.50000 | Val: loss = 1.54731, acc = 0.78600\n",
      "Training: loss = 1.62045, acc = 0.46429 | Val: loss = 1.54349, acc = 0.78600\n",
      "Training: loss = 1.51710, acc = 0.55714 | Val: loss = 1.53759, acc = 0.79400\n",
      "Training: loss = 1.60058, acc = 0.47857 | Val: loss = 1.53079, acc = 0.80000\n",
      "Training: loss = 1.49918, acc = 0.51429 | Val: loss = 1.52324, acc = 0.80200\n",
      "Training: loss = 1.52373, acc = 0.55714 | Val: loss = 1.51749, acc = 0.79800\n",
      "Training: loss = 1.42734, acc = 0.55714 | Val: loss = 1.50981, acc = 0.80200\n",
      "Training: loss = 1.42385, acc = 0.53571 | Val: loss = 1.50334, acc = 0.80000\n",
      "Training: loss = 1.52838, acc = 0.51429 | Val: loss = 1.49790, acc = 0.80400\n",
      "Training: loss = 1.53540, acc = 0.55000 | Val: loss = 1.49309, acc = 0.80800\n",
      "Training: loss = 1.52485, acc = 0.52857 | Val: loss = 1.48826, acc = 0.80800\n",
      "Training: loss = 1.49820, acc = 0.54286 | Val: loss = 1.48425, acc = 0.80400\n",
      "Training: loss = 1.51278, acc = 0.53571 | Val: loss = 1.48191, acc = 0.80600\n",
      "Training: loss = 1.44185, acc = 0.59286 | Val: loss = 1.47717, acc = 0.80800\n",
      "Training: loss = 1.55836, acc = 0.52857 | Val: loss = 1.47421, acc = 0.80000\n",
      "Training: loss = 1.53371, acc = 0.49286 | Val: loss = 1.47348, acc = 0.80000\n",
      "Training: loss = 1.48151, acc = 0.54286 | Val: loss = 1.47285, acc = 0.79600\n",
      "Training: loss = 1.43697, acc = 0.56429 | Val: loss = 1.47372, acc = 0.78800\n",
      "Training: loss = 1.52507, acc = 0.51429 | Val: loss = 1.47409, acc = 0.78400\n",
      "Training: loss = 1.52211, acc = 0.52143 | Val: loss = 1.47502, acc = 0.77800\n",
      "Training: loss = 1.39014, acc = 0.55000 | Val: loss = 1.47638, acc = 0.77800\n",
      "Training: loss = 1.51361, acc = 0.50714 | Val: loss = 1.47714, acc = 0.78200\n",
      "Training: loss = 1.48022, acc = 0.56429 | Val: loss = 1.47726, acc = 0.78400\n",
      "Training: loss = 1.52792, acc = 0.50000 | Val: loss = 1.47559, acc = 0.78800\n",
      "Training: loss = 1.40184, acc = 0.55714 | Val: loss = 1.47267, acc = 0.79400\n",
      "Training: loss = 1.38686, acc = 0.58571 | Val: loss = 1.46978, acc = 0.80000\n",
      "Training: loss = 1.53685, acc = 0.50000 | Val: loss = 1.46712, acc = 0.80000\n",
      "Training: loss = 1.42489, acc = 0.58571 | Val: loss = 1.46321, acc = 0.79800\n",
      "Training: loss = 1.44963, acc = 0.55000 | Val: loss = 1.45830, acc = 0.80200\n",
      "Training: loss = 1.47224, acc = 0.54286 | Val: loss = 1.45564, acc = 0.80400\n",
      "Training: loss = 1.44967, acc = 0.57857 | Val: loss = 1.45451, acc = 0.80600\n",
      "Training: loss = 1.49371, acc = 0.54286 | Val: loss = 1.45217, acc = 0.80200\n",
      "Training: loss = 1.50557, acc = 0.50000 | Val: loss = 1.44896, acc = 0.80200\n",
      "Training: loss = 1.57985, acc = 0.49286 | Val: loss = 1.44451, acc = 0.80400\n",
      "Training: loss = 1.53148, acc = 0.47857 | Val: loss = 1.43986, acc = 0.80600\n",
      "Training: loss = 1.49364, acc = 0.56429 | Val: loss = 1.43398, acc = 0.80600\n",
      "Training: loss = 1.52113, acc = 0.50714 | Val: loss = 1.43003, acc = 0.80800\n",
      "Training: loss = 1.45127, acc = 0.53571 | Val: loss = 1.42779, acc = 0.81000\n",
      "Training: loss = 1.51407, acc = 0.50000 | Val: loss = 1.42555, acc = 0.80800\n",
      "Training: loss = 1.48862, acc = 0.53571 | Val: loss = 1.42546, acc = 0.80800\n",
      "Training: loss = 1.50817, acc = 0.48571 | Val: loss = 1.42667, acc = 0.80400\n",
      "Training: loss = 1.51182, acc = 0.48571 | Val: loss = 1.42750, acc = 0.80200\n",
      "Training: loss = 1.47639, acc = 0.57143 | Val: loss = 1.42765, acc = 0.80000\n",
      "Training: loss = 1.48934, acc = 0.50000 | Val: loss = 1.42712, acc = 0.79200\n",
      "Training: loss = 1.42060, acc = 0.53571 | Val: loss = 1.42600, acc = 0.79400\n",
      "Training: loss = 1.36588, acc = 0.57857 | Val: loss = 1.42483, acc = 0.80000\n",
      "Training: loss = 1.47090, acc = 0.52143 | Val: loss = 1.42426, acc = 0.80000\n",
      "Training: loss = 1.50350, acc = 0.47857 | Val: loss = 1.42687, acc = 0.79400\n",
      "Training: loss = 1.37122, acc = 0.57857 | Val: loss = 1.42623, acc = 0.79000\n",
      "Training: loss = 1.56001, acc = 0.50714 | Val: loss = 1.42475, acc = 0.78600\n",
      "Training: loss = 1.47382, acc = 0.49286 | Val: loss = 1.42261, acc = 0.78600\n",
      "Training: loss = 1.53459, acc = 0.48571 | Val: loss = 1.42079, acc = 0.79000\n",
      "Training: loss = 1.53049, acc = 0.50000 | Val: loss = 1.41944, acc = 0.79000\n",
      "Training: loss = 1.43694, acc = 0.49286 | Val: loss = 1.42002, acc = 0.78800\n",
      "Training: loss = 1.51416, acc = 0.49286 | Val: loss = 1.42159, acc = 0.79000\n",
      "Training: loss = 1.57454, acc = 0.48571 | Val: loss = 1.42366, acc = 0.79200\n",
      "Training: loss = 1.44223, acc = 0.52857 | Val: loss = 1.42602, acc = 0.79600\n",
      "Training: loss = 1.51952, acc = 0.47143 | Val: loss = 1.42835, acc = 0.79200\n",
      "Training: loss = 1.44265, acc = 0.55000 | Val: loss = 1.43122, acc = 0.79200\n",
      "Training: loss = 1.57611, acc = 0.45000 | Val: loss = 1.43102, acc = 0.79200\n",
      "Training: loss = 1.47470, acc = 0.51429 | Val: loss = 1.42837, acc = 0.79000\n",
      "Training: loss = 1.48224, acc = 0.48571 | Val: loss = 1.42508, acc = 0.78800\n",
      "Training: loss = 1.35017, acc = 0.60000 | Val: loss = 1.42103, acc = 0.78600\n",
      "Training: loss = 1.43766, acc = 0.54286 | Val: loss = 1.41668, acc = 0.79000\n",
      "Training: loss = 1.53039, acc = 0.49286 | Val: loss = 1.41260, acc = 0.79200\n",
      "Training: loss = 1.44772, acc = 0.56429 | Val: loss = 1.40660, acc = 0.80200\n",
      "Training: loss = 1.47112, acc = 0.50714 | Val: loss = 1.40205, acc = 0.80400\n",
      "Training: loss = 1.50121, acc = 0.52143 | Val: loss = 1.39748, acc = 0.80400\n",
      "Training: loss = 1.37869, acc = 0.56429 | Val: loss = 1.39295, acc = 0.80200\n",
      "Training: loss = 1.46244, acc = 0.55000 | Val: loss = 1.38761, acc = 0.80800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.45882, acc = 0.55714 | Val: loss = 1.38440, acc = 0.81000\n",
      "Training: loss = 1.32593, acc = 0.57143 | Val: loss = 1.38321, acc = 0.80600\n",
      "Training: loss = 1.43962, acc = 0.51429 | Val: loss = 1.38376, acc = 0.80600\n",
      "Training: loss = 1.51672, acc = 0.53571 | Val: loss = 1.38550, acc = 0.80200\n",
      "Training: loss = 1.40054, acc = 0.57857 | Val: loss = 1.38748, acc = 0.80400\n",
      "Training: loss = 1.51710, acc = 0.49286 | Val: loss = 1.38719, acc = 0.80200\n",
      "Training: loss = 1.45004, acc = 0.55000 | Val: loss = 1.38632, acc = 0.79800\n",
      "Training: loss = 1.46456, acc = 0.53571 | Val: loss = 1.38441, acc = 0.79800\n",
      "Training: loss = 1.46434, acc = 0.54286 | Val: loss = 1.37991, acc = 0.80400\n",
      "Training: loss = 1.38389, acc = 0.59286 | Val: loss = 1.37676, acc = 0.80600\n",
      "Training: loss = 1.54054, acc = 0.52143 | Val: loss = 1.37523, acc = 0.80000\n",
      "Training: loss = 1.46665, acc = 0.52857 | Val: loss = 1.37317, acc = 0.79800\n",
      "Training: loss = 1.42501, acc = 0.57143 | Val: loss = 1.36910, acc = 0.80000\n",
      "Training: loss = 1.38655, acc = 0.57857 | Val: loss = 1.36696, acc = 0.80000\n",
      "Training: loss = 1.54952, acc = 0.50000 | Val: loss = 1.36640, acc = 0.80000\n",
      "Training: loss = 1.51869, acc = 0.52143 | Val: loss = 1.36601, acc = 0.80200\n",
      "Training: loss = 1.44096, acc = 0.56429 | Val: loss = 1.36657, acc = 0.80200\n",
      "Training: loss = 1.44645, acc = 0.54286 | Val: loss = 1.36972, acc = 0.79800\n",
      "Training: loss = 1.49598, acc = 0.52857 | Val: loss = 1.37506, acc = 0.79800\n",
      "Training: loss = 1.51519, acc = 0.51429 | Val: loss = 1.38032, acc = 0.79600\n",
      "Training: loss = 1.42575, acc = 0.55714 | Val: loss = 1.38512, acc = 0.79400\n",
      "Training: loss = 1.52038, acc = 0.53571 | Val: loss = 1.39059, acc = 0.78800\n",
      "Training: loss = 1.52677, acc = 0.52857 | Val: loss = 1.39354, acc = 0.78800\n",
      "Training: loss = 1.50014, acc = 0.49286 | Val: loss = 1.39379, acc = 0.78200\n",
      "Training: loss = 1.38586, acc = 0.56429 | Val: loss = 1.39138, acc = 0.78200\n",
      "Training: loss = 1.45834, acc = 0.54286 | Val: loss = 1.38672, acc = 0.78200\n",
      "Training: loss = 1.46978, acc = 0.51429 | Val: loss = 1.38112, acc = 0.78800\n",
      "Training: loss = 1.36628, acc = 0.56429 | Val: loss = 1.37710, acc = 0.79000\n",
      "Training: loss = 1.39320, acc = 0.55000 | Val: loss = 1.37344, acc = 0.78800\n",
      "Training: loss = 1.41817, acc = 0.56429 | Val: loss = 1.37319, acc = 0.78800\n",
      "Training: loss = 1.43759, acc = 0.53571 | Val: loss = 1.37264, acc = 0.78800\n",
      "Training: loss = 1.50324, acc = 0.50714 | Val: loss = 1.37294, acc = 0.78800\n",
      "Training: loss = 1.42699, acc = 0.55000 | Val: loss = 1.37328, acc = 0.79000\n",
      "Training: loss = 1.45196, acc = 0.58571 | Val: loss = 1.37238, acc = 0.79600\n",
      "Training: loss = 1.38916, acc = 0.62143 | Val: loss = 1.37257, acc = 0.79600\n",
      "Training: loss = 1.43617, acc = 0.57143 | Val: loss = 1.37153, acc = 0.80000\n",
      "Training: loss = 1.42802, acc = 0.55000 | Val: loss = 1.37238, acc = 0.80000\n",
      "Training: loss = 1.58244, acc = 0.47857 | Val: loss = 1.37085, acc = 0.80200\n",
      "Training: loss = 1.53012, acc = 0.50000 | Val: loss = 1.37129, acc = 0.80200\n",
      "Training: loss = 1.45702, acc = 0.53571 | Val: loss = 1.37263, acc = 0.79800\n",
      "Training: loss = 1.31465, acc = 0.58571 | Val: loss = 1.37295, acc = 0.80200\n",
      "Training: loss = 1.40024, acc = 0.55000 | Val: loss = 1.37280, acc = 0.79800\n",
      "Training: loss = 1.56075, acc = 0.51429 | Val: loss = 1.37159, acc = 0.79600\n",
      "Training: loss = 1.48387, acc = 0.51429 | Val: loss = 1.37017, acc = 0.79200\n",
      "Training: loss = 1.49108, acc = 0.55000 | Val: loss = 1.36597, acc = 0.78800\n",
      "Training: loss = 1.54978, acc = 0.48571 | Val: loss = 1.36495, acc = 0.79000\n",
      "Training: loss = 1.44705, acc = 0.52143 | Val: loss = 1.36406, acc = 0.79600\n",
      "Training: loss = 1.48720, acc = 0.57143 | Val: loss = 1.36200, acc = 0.79800\n",
      "Training: loss = 1.46047, acc = 0.57143 | Val: loss = 1.36222, acc = 0.79600\n",
      "Training: loss = 1.51342, acc = 0.54286 | Val: loss = 1.36382, acc = 0.79800\n",
      "Training: loss = 1.43938, acc = 0.56429 | Val: loss = 1.36621, acc = 0.79200\n",
      "Training: loss = 1.45000, acc = 0.52857 | Val: loss = 1.36914, acc = 0.79200\n",
      "Training: loss = 1.48462, acc = 0.47857 | Val: loss = 1.37207, acc = 0.79600\n",
      "Training: loss = 1.38564, acc = 0.59286 | Val: loss = 1.37447, acc = 0.79000\n",
      "Training: loss = 1.43842, acc = 0.52857 | Val: loss = 1.37490, acc = 0.78600\n",
      "Training: loss = 1.50913, acc = 0.52143 | Val: loss = 1.37471, acc = 0.78800\n",
      "Training: loss = 1.49936, acc = 0.50714 | Val: loss = 1.37333, acc = 0.79000\n",
      "Training: loss = 1.42293, acc = 0.57143 | Val: loss = 1.37064, acc = 0.78800\n",
      "Training: loss = 1.44512, acc = 0.57143 | Val: loss = 1.36712, acc = 0.78800\n",
      "Training: loss = 1.51594, acc = 0.53571 | Val: loss = 1.36562, acc = 0.79200\n",
      "Training: loss = 1.41965, acc = 0.54286 | Val: loss = 1.36343, acc = 0.79200\n",
      "Training: loss = 1.27978, acc = 0.62143 | Val: loss = 1.35980, acc = 0.79400\n",
      "Training: loss = 1.48398, acc = 0.53571 | Val: loss = 1.35749, acc = 0.79800\n",
      "Training: loss = 1.48809, acc = 0.50000 | Val: loss = 1.35487, acc = 0.80000\n",
      "Training: loss = 1.35505, acc = 0.60714 | Val: loss = 1.35358, acc = 0.80000\n",
      "Training: loss = 1.54325, acc = 0.46429 | Val: loss = 1.35185, acc = 0.80000\n",
      "Training: loss = 1.42195, acc = 0.52857 | Val: loss = 1.34859, acc = 0.79800\n",
      "Training: loss = 1.44303, acc = 0.61429 | Val: loss = 1.34539, acc = 0.80000\n",
      "Training: loss = 1.55758, acc = 0.48571 | Val: loss = 1.34155, acc = 0.80200\n",
      "Training: loss = 1.43128, acc = 0.57857 | Val: loss = 1.33726, acc = 0.80000\n",
      "Training: loss = 1.34995, acc = 0.59286 | Val: loss = 1.33277, acc = 0.80200\n",
      "Training: loss = 1.44397, acc = 0.59286 | Val: loss = 1.32940, acc = 0.80800\n",
      "Training: loss = 1.48246, acc = 0.52857 | Val: loss = 1.32497, acc = 0.81200\n",
      "Training: loss = 1.42498, acc = 0.52143 | Val: loss = 1.32171, acc = 0.81200\n",
      "Training: loss = 1.55289, acc = 0.48571 | Val: loss = 1.32014, acc = 0.81000\n",
      "Training: loss = 1.50170, acc = 0.53571 | Val: loss = 1.32009, acc = 0.81000\n",
      "Training: loss = 1.45138, acc = 0.55714 | Val: loss = 1.32105, acc = 0.79800\n",
      "Training: loss = 1.39929, acc = 0.52143 | Val: loss = 1.32304, acc = 0.80200\n",
      "Training: loss = 1.52319, acc = 0.50714 | Val: loss = 1.32614, acc = 0.79800\n",
      "Training: loss = 1.40572, acc = 0.57857 | Val: loss = 1.33115, acc = 0.79200\n",
      "Training: loss = 1.40796, acc = 0.57857 | Val: loss = 1.33486, acc = 0.79000\n",
      "Training: loss = 1.38692, acc = 0.55714 | Val: loss = 1.33759, acc = 0.78800\n",
      "Training: loss = 1.41546, acc = 0.53571 | Val: loss = 1.33837, acc = 0.78800\n",
      "Training: loss = 1.55088, acc = 0.52857 | Val: loss = 1.33900, acc = 0.79000\n",
      "Training: loss = 1.51891, acc = 0.45714 | Val: loss = 1.33922, acc = 0.79200\n",
      "Training: loss = 1.42846, acc = 0.55000 | Val: loss = 1.33974, acc = 0.79400\n",
      "Training: loss = 1.31552, acc = 0.59286 | Val: loss = 1.34178, acc = 0.79200\n",
      "Training: loss = 1.35819, acc = 0.55000 | Val: loss = 1.34308, acc = 0.79200\n",
      "Training: loss = 1.37416, acc = 0.56429 | Val: loss = 1.34548, acc = 0.79400\n",
      "Training: loss = 1.35470, acc = 0.55000 | Val: loss = 1.34786, acc = 0.79800\n",
      "Training: loss = 1.35751, acc = 0.58571 | Val: loss = 1.34641, acc = 0.80000\n",
      "Training: loss = 1.46831, acc = 0.50714 | Val: loss = 1.34577, acc = 0.80000\n",
      "Training: loss = 1.51708, acc = 0.55714 | Val: loss = 1.34452, acc = 0.79800\n",
      "Training: loss = 1.42645, acc = 0.50714 | Val: loss = 1.33998, acc = 0.79600\n",
      "Training: loss = 1.46425, acc = 0.55714 | Val: loss = 1.33681, acc = 0.80000\n",
      "Training: loss = 1.48898, acc = 0.53571 | Val: loss = 1.33200, acc = 0.80200\n",
      "Training: loss = 1.47810, acc = 0.53571 | Val: loss = 1.32520, acc = 0.80000\n",
      "Training: loss = 1.51810, acc = 0.51429 | Val: loss = 1.31706, acc = 0.80200\n",
      "Training: loss = 1.40445, acc = 0.60000 | Val: loss = 1.31025, acc = 0.81000\n",
      "Training: loss = 1.47724, acc = 0.56429 | Val: loss = 1.30484, acc = 0.80600\n",
      "Training: loss = 1.41706, acc = 0.56429 | Val: loss = 1.30354, acc = 0.80600\n",
      "Training: loss = 1.34899, acc = 0.61429 | Val: loss = 1.30699, acc = 0.80200\n",
      "Training: loss = 1.42558, acc = 0.61429 | Val: loss = 1.31200, acc = 0.79800\n",
      "Training: loss = 1.50760, acc = 0.48571 | Val: loss = 1.31945, acc = 0.79800\n",
      "Training: loss = 1.51861, acc = 0.53571 | Val: loss = 1.32610, acc = 0.79400\n",
      "Training: loss = 1.46627, acc = 0.55000 | Val: loss = 1.33197, acc = 0.79000\n",
      "Training: loss = 1.40155, acc = 0.57143 | Val: loss = 1.33578, acc = 0.79200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.41184, acc = 0.57857 | Val: loss = 1.33667, acc = 0.79400\n",
      "Training: loss = 1.41444, acc = 0.58571 | Val: loss = 1.33487, acc = 0.79200\n",
      "Training: loss = 1.44967, acc = 0.50714 | Val: loss = 1.33267, acc = 0.78800\n",
      "Training: loss = 1.39094, acc = 0.50000 | Val: loss = 1.32986, acc = 0.78400\n",
      "Training: loss = 1.45070, acc = 0.50714 | Val: loss = 1.32638, acc = 0.79200\n",
      "Training: loss = 1.54495, acc = 0.51429 | Val: loss = 1.32547, acc = 0.79200\n",
      "Training: loss = 1.42511, acc = 0.59286 | Val: loss = 1.32401, acc = 0.79000\n",
      "Training: loss = 1.49901, acc = 0.50000 | Val: loss = 1.32212, acc = 0.79400\n",
      "Training: loss = 1.40820, acc = 0.55000 | Val: loss = 1.32002, acc = 0.79600\n",
      "Training: loss = 1.46951, acc = 0.55714 | Val: loss = 1.31844, acc = 0.79600\n",
      "Training: loss = 1.54517, acc = 0.52143 | Val: loss = 1.31793, acc = 0.79600\n",
      "Training: loss = 1.44818, acc = 0.57143 | Val: loss = 1.31755, acc = 0.79800\n",
      "Training: loss = 1.41179, acc = 0.57143 | Val: loss = 1.31565, acc = 0.80200\n",
      "Training: loss = 1.41764, acc = 0.55000 | Val: loss = 1.31263, acc = 0.79800\n",
      "Training: loss = 1.49142, acc = 0.52857 | Val: loss = 1.30943, acc = 0.80400\n",
      "Training: loss = 1.36171, acc = 0.57857 | Val: loss = 1.30724, acc = 0.80800\n",
      "Training: loss = 1.48717, acc = 0.57143 | Val: loss = 1.30761, acc = 0.80600\n",
      "Training: loss = 1.43846, acc = 0.57143 | Val: loss = 1.30754, acc = 0.80200\n",
      "Training: loss = 1.35489, acc = 0.61429 | Val: loss = 1.30934, acc = 0.79800\n",
      "Training: loss = 1.55988, acc = 0.46429 | Val: loss = 1.31161, acc = 0.79400\n",
      "Training: loss = 1.50156, acc = 0.52857 | Val: loss = 1.31227, acc = 0.79600\n",
      "Training: loss = 1.34392, acc = 0.58571 | Val: loss = 1.31342, acc = 0.79800\n",
      "Training: loss = 1.40435, acc = 0.59286 | Val: loss = 1.31415, acc = 0.80000\n",
      "Training: loss = 1.34344, acc = 0.65714 | Val: loss = 1.31584, acc = 0.80400\n",
      "Training: loss = 1.46539, acc = 0.51429 | Val: loss = 1.31593, acc = 0.80200\n",
      "Training: loss = 1.26750, acc = 0.64286 | Val: loss = 1.31764, acc = 0.80000\n",
      "Training: loss = 1.43313, acc = 0.54286 | Val: loss = 1.32011, acc = 0.80000\n",
      "Training: loss = 1.54587, acc = 0.55714 | Val: loss = 1.32267, acc = 0.79600\n",
      "Training: loss = 1.38812, acc = 0.55714 | Val: loss = 1.32535, acc = 0.79000\n",
      "Training: loss = 1.52583, acc = 0.52143 | Val: loss = 1.32767, acc = 0.79000\n",
      "Training: loss = 1.56525, acc = 0.49286 | Val: loss = 1.32605, acc = 0.79400\n",
      "Training: loss = 1.47233, acc = 0.53571 | Val: loss = 1.32384, acc = 0.79200\n",
      "Training: loss = 1.50679, acc = 0.47857 | Val: loss = 1.32288, acc = 0.79000\n",
      "Training: loss = 1.45905, acc = 0.50714 | Val: loss = 1.32223, acc = 0.80000\n",
      "Training: loss = 1.34462, acc = 0.65000 | Val: loss = 1.32082, acc = 0.80200\n",
      "Training: loss = 1.40104, acc = 0.60000 | Val: loss = 1.31818, acc = 0.80600\n",
      "Training: loss = 1.44081, acc = 0.54286 | Val: loss = 1.31647, acc = 0.80400\n",
      "Training: loss = 1.41913, acc = 0.52857 | Val: loss = 1.31386, acc = 0.80400\n",
      "Training: loss = 1.35752, acc = 0.56429 | Val: loss = 1.31321, acc = 0.80200\n",
      "Training: loss = 1.38166, acc = 0.57143 | Val: loss = 1.31260, acc = 0.79800\n",
      "Training: loss = 1.37112, acc = 0.57857 | Val: loss = 1.31077, acc = 0.79800\n",
      "Training: loss = 1.53850, acc = 0.45714 | Val: loss = 1.30852, acc = 0.80000\n",
      "Training: loss = 1.36614, acc = 0.53571 | Val: loss = 1.30873, acc = 0.79600\n",
      "Training: loss = 1.44305, acc = 0.50000 | Val: loss = 1.30989, acc = 0.79800\n",
      "Training: loss = 1.48765, acc = 0.57143 | Val: loss = 1.31041, acc = 0.80200\n",
      "Training: loss = 1.40847, acc = 0.58571 | Val: loss = 1.31304, acc = 0.79800\n",
      "Training: loss = 1.40416, acc = 0.56429 | Val: loss = 1.31518, acc = 0.80000\n",
      "Training: loss = 1.45728, acc = 0.57857 | Val: loss = 1.31575, acc = 0.79800\n",
      "Training: loss = 1.39740, acc = 0.56429 | Val: loss = 1.31550, acc = 0.80600\n",
      "Training: loss = 1.42238, acc = 0.52857 | Val: loss = 1.31469, acc = 0.80600\n",
      "Training: loss = 1.34639, acc = 0.62857 | Val: loss = 1.31375, acc = 0.80200\n",
      "Training: loss = 1.41141, acc = 0.59286 | Val: loss = 1.31406, acc = 0.79600\n",
      "Training: loss = 1.50527, acc = 0.55000 | Val: loss = 1.31455, acc = 0.79400\n",
      "Training: loss = 1.33725, acc = 0.61429 | Val: loss = 1.31340, acc = 0.79200\n",
      "Training: loss = 1.46582, acc = 0.51429 | Val: loss = 1.31211, acc = 0.79400\n",
      "Training: loss = 1.35972, acc = 0.60000 | Val: loss = 1.31055, acc = 0.79200\n",
      "Training: loss = 1.45221, acc = 0.53571 | Val: loss = 1.31053, acc = 0.79000\n",
      "Training: loss = 1.38048, acc = 0.58571 | Val: loss = 1.30808, acc = 0.79200\n",
      "Training: loss = 1.41780, acc = 0.60714 | Val: loss = 1.30501, acc = 0.79200\n",
      "Training: loss = 1.50443, acc = 0.51429 | Val: loss = 1.29971, acc = 0.79200\n",
      "Training: loss = 1.58090, acc = 0.48571 | Val: loss = 1.29436, acc = 0.80000\n",
      "Training: loss = 1.29074, acc = 0.58571 | Val: loss = 1.28951, acc = 0.80400\n",
      "Training: loss = 1.42298, acc = 0.55000 | Val: loss = 1.28661, acc = 0.80400\n",
      "Training: loss = 1.41993, acc = 0.59286 | Val: loss = 1.28473, acc = 0.80000\n",
      "Training: loss = 1.55568, acc = 0.52857 | Val: loss = 1.28524, acc = 0.79800\n",
      "Training: loss = 1.40926, acc = 0.60000 | Val: loss = 1.28838, acc = 0.79800\n",
      "Training: loss = 1.31371, acc = 0.59286 | Val: loss = 1.29228, acc = 0.80400\n",
      "Training: loss = 1.51699, acc = 0.49286 | Val: loss = 1.29729, acc = 0.79800\n",
      "Training: loss = 1.28102, acc = 0.62143 | Val: loss = 1.30159, acc = 0.79800\n",
      "Training: loss = 1.52661, acc = 0.47857 | Val: loss = 1.30466, acc = 0.79600\n",
      "Training: loss = 1.53063, acc = 0.52857 | Val: loss = 1.30890, acc = 0.79400\n",
      "Training: loss = 1.46558, acc = 0.57143 | Val: loss = 1.31400, acc = 0.79800\n",
      "Training: loss = 1.43062, acc = 0.61429 | Val: loss = 1.31637, acc = 0.79400\n",
      "Training: loss = 1.45963, acc = 0.57857 | Val: loss = 1.32062, acc = 0.79600\n",
      "Training: loss = 1.40215, acc = 0.62143 | Val: loss = 1.32300, acc = 0.79800\n",
      "Training: loss = 1.33719, acc = 0.58571 | Val: loss = 1.32474, acc = 0.80000\n",
      "Training: loss = 1.45158, acc = 0.57857 | Val: loss = 1.32466, acc = 0.80000\n",
      "Training: loss = 1.48688, acc = 0.55000 | Val: loss = 1.32426, acc = 0.80000\n",
      "Training: loss = 1.26764, acc = 0.65000 | Val: loss = 1.32104, acc = 0.79600\n",
      "Training: loss = 1.42176, acc = 0.59286 | Val: loss = 1.31747, acc = 0.79600\n",
      "Training: loss = 1.40565, acc = 0.54286 | Val: loss = 1.31390, acc = 0.79600\n",
      "Training: loss = 1.43299, acc = 0.58571 | Val: loss = 1.31098, acc = 0.79600\n",
      "Training: loss = 1.42438, acc = 0.52143 | Val: loss = 1.30951, acc = 0.79600\n",
      "Training: loss = 1.53420, acc = 0.50000 | Val: loss = 1.30628, acc = 0.79800\n",
      "Training: loss = 1.50160, acc = 0.55714 | Val: loss = 1.30406, acc = 0.79600\n",
      "Training: loss = 1.30498, acc = 0.63571 | Val: loss = 1.30161, acc = 0.79600\n",
      "Training: loss = 1.39153, acc = 0.56429 | Val: loss = 1.29796, acc = 0.79600\n",
      "Training: loss = 1.35875, acc = 0.58571 | Val: loss = 1.29519, acc = 0.79800\n",
      "Training: loss = 1.44875, acc = 0.57857 | Val: loss = 1.29340, acc = 0.80000\n",
      "Training: loss = 1.38320, acc = 0.58571 | Val: loss = 1.29292, acc = 0.79800\n",
      "Training: loss = 1.43404, acc = 0.57143 | Val: loss = 1.29329, acc = 0.80000\n",
      "Training: loss = 1.35931, acc = 0.55000 | Val: loss = 1.29470, acc = 0.79200\n",
      "Training: loss = 1.38167, acc = 0.56429 | Val: loss = 1.29741, acc = 0.79200\n",
      "Training: loss = 1.44441, acc = 0.56429 | Val: loss = 1.29923, acc = 0.79800\n",
      "Training: loss = 1.37622, acc = 0.58571 | Val: loss = 1.30416, acc = 0.80000\n",
      "Training: loss = 1.40208, acc = 0.61429 | Val: loss = 1.30873, acc = 0.79600\n",
      "Training: loss = 1.36475, acc = 0.57857 | Val: loss = 1.31295, acc = 0.79200\n",
      "Training: loss = 1.40582, acc = 0.57857 | Val: loss = 1.31662, acc = 0.79200\n",
      "Training: loss = 1.44559, acc = 0.61429 | Val: loss = 1.31587, acc = 0.79400\n",
      "Training: loss = 1.40427, acc = 0.53571 | Val: loss = 1.31423, acc = 0.79600\n",
      "Training: loss = 1.47854, acc = 0.55714 | Val: loss = 1.31263, acc = 0.79800\n",
      "Training: loss = 1.39458, acc = 0.58571 | Val: loss = 1.31149, acc = 0.79800\n",
      "Training: loss = 1.51411, acc = 0.51429 | Val: loss = 1.30706, acc = 0.79800\n",
      "Training: loss = 1.51549, acc = 0.51429 | Val: loss = 1.30015, acc = 0.80200\n",
      "Training: loss = 1.48720, acc = 0.52857 | Val: loss = 1.29429, acc = 0.80000\n",
      "Training: loss = 1.35678, acc = 0.55714 | Val: loss = 1.28985, acc = 0.80200\n",
      "Training: loss = 1.40596, acc = 0.55000 | Val: loss = 1.28683, acc = 0.80200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.45751, acc = 0.51429 | Val: loss = 1.28547, acc = 0.79600\n",
      "Training: loss = 1.41425, acc = 0.58571 | Val: loss = 1.28595, acc = 0.80400\n",
      "Training: loss = 1.49366, acc = 0.55000 | Val: loss = 1.28618, acc = 0.80400\n",
      "Training: loss = 1.43489, acc = 0.55000 | Val: loss = 1.28727, acc = 0.80200\n",
      "Training: loss = 1.39225, acc = 0.55000 | Val: loss = 1.28871, acc = 0.80000\n",
      "Training: loss = 1.48114, acc = 0.52857 | Val: loss = 1.29026, acc = 0.79600\n",
      "Training: loss = 1.33985, acc = 0.59286 | Val: loss = 1.29186, acc = 0.79800\n",
      "Training: loss = 1.41084, acc = 0.56429 | Val: loss = 1.29592, acc = 0.79800\n",
      "Training: loss = 1.41240, acc = 0.57857 | Val: loss = 1.30089, acc = 0.79400\n",
      "Training: loss = 1.34941, acc = 0.60000 | Val: loss = 1.30682, acc = 0.79400\n",
      "Training: loss = 1.43013, acc = 0.58571 | Val: loss = 1.31327, acc = 0.79600\n",
      "Training: loss = 1.47640, acc = 0.52857 | Val: loss = 1.31962, acc = 0.79400\n",
      "Training: loss = 1.47604, acc = 0.57143 | Val: loss = 1.32606, acc = 0.79400\n",
      "Training: loss = 1.35809, acc = 0.61429 | Val: loss = 1.33012, acc = 0.78800\n",
      "Training: loss = 1.44711, acc = 0.58571 | Val: loss = 1.32815, acc = 0.78400\n",
      "Training: loss = 1.48239, acc = 0.57143 | Val: loss = 1.32494, acc = 0.78400\n",
      "Training: loss = 1.41648, acc = 0.54286 | Val: loss = 1.31820, acc = 0.78600\n",
      "Training: loss = 1.46956, acc = 0.55000 | Val: loss = 1.30886, acc = 0.79000\n",
      "Training: loss = 1.41826, acc = 0.55000 | Val: loss = 1.30061, acc = 0.79200\n",
      "Training: loss = 1.44297, acc = 0.52857 | Val: loss = 1.29210, acc = 0.79400\n",
      "Training: loss = 1.49048, acc = 0.54286 | Val: loss = 1.28411, acc = 0.79200\n",
      "Training: loss = 1.44091, acc = 0.55714 | Val: loss = 1.27860, acc = 0.79000\n",
      "Training: loss = 1.38660, acc = 0.58571 | Val: loss = 1.27133, acc = 0.79600\n",
      "Training: loss = 1.47836, acc = 0.50714 | Val: loss = 1.26526, acc = 0.80000\n",
      "Training: loss = 1.41942, acc = 0.58571 | Val: loss = 1.25973, acc = 0.80200\n",
      "Training: loss = 1.49980, acc = 0.52143 | Val: loss = 1.25616, acc = 0.80200\n",
      "Training: loss = 1.40742, acc = 0.55000 | Val: loss = 1.25574, acc = 0.80200\n",
      "Training: loss = 1.45043, acc = 0.53571 | Val: loss = 1.25389, acc = 0.81200\n",
      "Training: loss = 1.47348, acc = 0.52143 | Val: loss = 1.25235, acc = 0.81600\n",
      "Training: loss = 1.37217, acc = 0.57857 | Val: loss = 1.25166, acc = 0.81600\n",
      "Training: loss = 1.46759, acc = 0.52857 | Val: loss = 1.25416, acc = 0.81800\n",
      "Training: loss = 1.43663, acc = 0.53571 | Val: loss = 1.25902, acc = 0.82400\n",
      "Training: loss = 1.45071, acc = 0.59286 | Val: loss = 1.26339, acc = 0.81800\n",
      "Training: loss = 1.50141, acc = 0.53571 | Val: loss = 1.26825, acc = 0.81600\n",
      "Training: loss = 1.26541, acc = 0.62143 | Val: loss = 1.27377, acc = 0.80800\n",
      "Training: loss = 1.47031, acc = 0.57857 | Val: loss = 1.28027, acc = 0.80400\n",
      "Training: loss = 1.46916, acc = 0.47857 | Val: loss = 1.28411, acc = 0.80200\n",
      "Training: loss = 1.42572, acc = 0.55714 | Val: loss = 1.28847, acc = 0.80200\n",
      "Training: loss = 1.44546, acc = 0.49286 | Val: loss = 1.29034, acc = 0.80600\n",
      "Training: loss = 1.56499, acc = 0.47857 | Val: loss = 1.29096, acc = 0.81000\n",
      "Training: loss = 1.37325, acc = 0.57143 | Val: loss = 1.29067, acc = 0.80800\n",
      "Training: loss = 1.50009, acc = 0.52857 | Val: loss = 1.29024, acc = 0.80000\n",
      "Training: loss = 1.43145, acc = 0.57143 | Val: loss = 1.28941, acc = 0.80400\n",
      "Training: loss = 1.45427, acc = 0.56429 | Val: loss = 1.28786, acc = 0.79800\n",
      "Training: loss = 1.32560, acc = 0.63571 | Val: loss = 1.28616, acc = 0.79800\n",
      "Training: loss = 1.36509, acc = 0.64286 | Val: loss = 1.28231, acc = 0.80000\n",
      "Training: loss = 1.45804, acc = 0.56429 | Val: loss = 1.27789, acc = 0.80000\n",
      "Training: loss = 1.43659, acc = 0.57857 | Val: loss = 1.27032, acc = 0.79600\n",
      "Training: loss = 1.45647, acc = 0.50714 | Val: loss = 1.26115, acc = 0.80200\n",
      "Training: loss = 1.40838, acc = 0.55714 | Val: loss = 1.25376, acc = 0.81200\n",
      "Training: loss = 1.50374, acc = 0.56429 | Val: loss = 1.24755, acc = 0.81400\n",
      "Training: loss = 1.40198, acc = 0.60714 | Val: loss = 1.24288, acc = 0.81800\n",
      "Training: loss = 1.47166, acc = 0.56429 | Val: loss = 1.24098, acc = 0.81400\n",
      "Training: loss = 1.37542, acc = 0.57143 | Val: loss = 1.24101, acc = 0.81400\n",
      "Training: loss = 1.52009, acc = 0.48571 | Val: loss = 1.24300, acc = 0.81800\n",
      "Training: loss = 1.47264, acc = 0.53571 | Val: loss = 1.24566, acc = 0.81600\n",
      "Training: loss = 1.44112, acc = 0.56429 | Val: loss = 1.24928, acc = 0.81000\n",
      "Training: loss = 1.50230, acc = 0.54286 | Val: loss = 1.25215, acc = 0.80800\n",
      "Training: loss = 1.39395, acc = 0.58571 | Val: loss = 1.25481, acc = 0.80600\n",
      "Training: loss = 1.47269, acc = 0.53571 | Val: loss = 1.25800, acc = 0.80200\n",
      "Training: loss = 1.45779, acc = 0.58571 | Val: loss = 1.25973, acc = 0.80800\n",
      "Training: loss = 1.44254, acc = 0.52857 | Val: loss = 1.26300, acc = 0.80800\n",
      "Training: loss = 1.35220, acc = 0.57143 | Val: loss = 1.26670, acc = 0.80400\n",
      "Training: loss = 1.38699, acc = 0.57857 | Val: loss = 1.27025, acc = 0.80800\n",
      "Training: loss = 1.47008, acc = 0.52857 | Val: loss = 1.27389, acc = 0.80400\n",
      "Training: loss = 1.52581, acc = 0.51429 | Val: loss = 1.27871, acc = 0.80400\n",
      "Training: loss = 1.45462, acc = 0.53571 | Val: loss = 1.28030, acc = 0.80600\n",
      "Training: loss = 1.37293, acc = 0.56429 | Val: loss = 1.28150, acc = 0.80800\n",
      "Training: loss = 1.44550, acc = 0.52857 | Val: loss = 1.28217, acc = 0.80600\n",
      "Training: loss = 1.43599, acc = 0.57143 | Val: loss = 1.28162, acc = 0.80400\n",
      "Training: loss = 1.50744, acc = 0.56429 | Val: loss = 1.27906, acc = 0.80400\n",
      "Training: loss = 1.35856, acc = 0.56429 | Val: loss = 1.27747, acc = 0.80200\n",
      "Training: loss = 1.46572, acc = 0.55000 | Val: loss = 1.27393, acc = 0.80200\n",
      "Training: loss = 1.40358, acc = 0.59286 | Val: loss = 1.27100, acc = 0.80000\n",
      "Training: loss = 1.44163, acc = 0.57143 | Val: loss = 1.26869, acc = 0.79800\n",
      "Training: loss = 1.36062, acc = 0.57143 | Val: loss = 1.26599, acc = 0.80000\n",
      "Training: loss = 1.47648, acc = 0.52143 | Val: loss = 1.26394, acc = 0.79800\n",
      "Training: loss = 1.38235, acc = 0.61429 | Val: loss = 1.26164, acc = 0.80400\n",
      "Training: loss = 1.55351, acc = 0.52857 | Val: loss = 1.26246, acc = 0.81000\n",
      "Training: loss = 1.36099, acc = 0.60714 | Val: loss = 1.26427, acc = 0.80600\n",
      "Training: loss = 1.46292, acc = 0.55714 | Val: loss = 1.26676, acc = 0.80400\n",
      "Training: loss = 1.38096, acc = 0.55714 | Val: loss = 1.26920, acc = 0.80000\n",
      "Training: loss = 1.48007, acc = 0.56429 | Val: loss = 1.27093, acc = 0.79800\n",
      "Training: loss = 1.42694, acc = 0.56429 | Val: loss = 1.27341, acc = 0.79800\n",
      "Training: loss = 1.32599, acc = 0.60714 | Val: loss = 1.27711, acc = 0.79600\n",
      "Training: loss = 1.30513, acc = 0.60714 | Val: loss = 1.28252, acc = 0.79200\n",
      "Training: loss = 1.43149, acc = 0.57857 | Val: loss = 1.29138, acc = 0.78600\n",
      "Training: loss = 1.36773, acc = 0.60714 | Val: loss = 1.29944, acc = 0.78600\n",
      "Training: loss = 1.36913, acc = 0.58571 | Val: loss = 1.30582, acc = 0.79000\n",
      "Training: loss = 1.36350, acc = 0.56429 | Val: loss = 1.30951, acc = 0.78600\n",
      "Training: loss = 1.34823, acc = 0.59286 | Val: loss = 1.31143, acc = 0.78600\n",
      "Training: loss = 1.50763, acc = 0.50000 | Val: loss = 1.31031, acc = 0.78600\n",
      "Training: loss = 1.45935, acc = 0.51429 | Val: loss = 1.30813, acc = 0.79000\n",
      "Training: loss = 1.37610, acc = 0.57143 | Val: loss = 1.30583, acc = 0.79000\n",
      "Training: loss = 1.33544, acc = 0.58571 | Val: loss = 1.30198, acc = 0.79000\n",
      "Training: loss = 1.38446, acc = 0.62143 | Val: loss = 1.29571, acc = 0.79400\n",
      "Training: loss = 1.43061, acc = 0.58571 | Val: loss = 1.29084, acc = 0.79000\n",
      "Training: loss = 1.40318, acc = 0.57143 | Val: loss = 1.28603, acc = 0.79200\n",
      "Training: loss = 1.41039, acc = 0.61429 | Val: loss = 1.27975, acc = 0.79800\n",
      "Training: loss = 1.34447, acc = 0.58571 | Val: loss = 1.27613, acc = 0.80200\n",
      "Training: loss = 1.53651, acc = 0.55714 | Val: loss = 1.27252, acc = 0.79600\n",
      "Training: loss = 1.44332, acc = 0.58571 | Val: loss = 1.26804, acc = 0.79800\n",
      "Training: loss = 1.38386, acc = 0.59286 | Val: loss = 1.26399, acc = 0.79800\n",
      "Training: loss = 1.37461, acc = 0.60714 | Val: loss = 1.25999, acc = 0.79600\n",
      "Training: loss = 1.49130, acc = 0.52857 | Val: loss = 1.25625, acc = 0.80200\n",
      "Training: loss = 1.29761, acc = 0.59286 | Val: loss = 1.25439, acc = 0.80400\n",
      "Training: loss = 1.46818, acc = 0.50000 | Val: loss = 1.25369, acc = 0.80600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: loss = 1.37054, acc = 0.55714 | Val: loss = 1.25286, acc = 0.80600\n",
      "Training: loss = 1.34573, acc = 0.57857 | Val: loss = 1.25447, acc = 0.80800\n",
      "Training: loss = 1.44390, acc = 0.54286 | Val: loss = 1.25428, acc = 0.80400\n",
      "Training: loss = 1.42971, acc = 0.60000 | Val: loss = 1.25423, acc = 0.80200\n",
      "Training: loss = 1.38067, acc = 0.55000 | Val: loss = 1.25530, acc = 0.80600\n",
      "Training: loss = 1.37832, acc = 0.55714 | Val: loss = 1.25643, acc = 0.80800\n",
      "Training: loss = 1.30024, acc = 0.60714 | Val: loss = 1.25814, acc = 0.80800\n",
      "Training: loss = 1.49375, acc = 0.53571 | Val: loss = 1.25917, acc = 0.80800\n",
      "Training: loss = 1.49031, acc = 0.52857 | Val: loss = 1.26032, acc = 0.80800\n",
      "Training: loss = 1.33994, acc = 0.57143 | Val: loss = 1.26318, acc = 0.80000\n",
      "Training: loss = 1.51041, acc = 0.47857 | Val: loss = 1.26786, acc = 0.79800\n",
      "Training: loss = 1.39380, acc = 0.57143 | Val: loss = 1.27159, acc = 0.79400\n",
      "Training: loss = 1.51471, acc = 0.52857 | Val: loss = 1.27285, acc = 0.79600\n",
      "Training: loss = 1.42024, acc = 0.57857 | Val: loss = 1.27111, acc = 0.79600\n",
      "Training: loss = 1.44422, acc = 0.57857 | Val: loss = 1.27136, acc = 0.79600\n",
      "Training: loss = 1.34089, acc = 0.63571 | Val: loss = 1.27172, acc = 0.79200\n",
      "Training: loss = 1.50692, acc = 0.52143 | Val: loss = 1.26955, acc = 0.79200\n",
      "Training: loss = 1.36514, acc = 0.57857 | Val: loss = 1.26793, acc = 0.79400\n",
      "Training: loss = 1.44471, acc = 0.53571 | Val: loss = 1.26652, acc = 0.79000\n",
      "Training: loss = 1.41051, acc = 0.58571 | Val: loss = 1.26647, acc = 0.78800\n",
      "Training: loss = 1.49406, acc = 0.52857 | Val: loss = 1.26929, acc = 0.78800\n",
      "Training: loss = 1.50957, acc = 0.50000 | Val: loss = 1.27200, acc = 0.78600\n",
      "Training: loss = 1.44472, acc = 0.57143 | Val: loss = 1.27133, acc = 0.78800\n",
      "Training: loss = 1.43094, acc = 0.55000 | Val: loss = 1.26960, acc = 0.79400\n",
      "Training: loss = 1.45128, acc = 0.51429 | Val: loss = 1.26961, acc = 0.79200\n",
      "Training: loss = 1.45400, acc = 0.57143 | Val: loss = 1.26796, acc = 0.79000\n",
      "Training: loss = 1.30113, acc = 0.60000 | Val: loss = 1.26665, acc = 0.78800\n",
      "Training: loss = 1.48999, acc = 0.52143 | Val: loss = 1.26617, acc = 0.79600\n",
      "Training: loss = 1.43982, acc = 0.57857 | Val: loss = 1.26574, acc = 0.80200\n",
      "Training: loss = 1.31992, acc = 0.59286 | Val: loss = 1.26711, acc = 0.81000\n",
      "Training: loss = 1.32093, acc = 0.60714 | Val: loss = 1.26947, acc = 0.81400\n",
      "Training: loss = 1.47888, acc = 0.55000 | Val: loss = 1.27310, acc = 0.80800\n",
      "Training: loss = 1.65455, acc = 0.50000 | Val: loss = 1.27277, acc = 0.80800\n",
      "Training: loss = 1.37819, acc = 0.56429 | Val: loss = 1.27403, acc = 0.80200\n",
      "Training: loss = 1.42654, acc = 0.54286 | Val: loss = 1.27408, acc = 0.80200\n",
      "Training: loss = 1.41607, acc = 0.61429 | Val: loss = 1.27433, acc = 0.79800\n",
      "Training: loss = 1.45201, acc = 0.58571 | Val: loss = 1.27350, acc = 0.79600\n",
      "Training: loss = 1.32280, acc = 0.62143 | Val: loss = 1.27152, acc = 0.79600\n",
      "Training: loss = 1.27317, acc = 0.64286 | Val: loss = 1.27138, acc = 0.79800\n",
      "Training: loss = 1.41481, acc = 0.56429 | Val: loss = 1.27179, acc = 0.79400\n",
      "Training: loss = 1.37620, acc = 0.56429 | Val: loss = 1.27321, acc = 0.79400\n",
      "Training: loss = 1.43427, acc = 0.57143 | Val: loss = 1.27357, acc = 0.79200\n",
      "Training: loss = 1.50772, acc = 0.51429 | Val: loss = 1.27209, acc = 0.78800\n",
      "Training: loss = 1.43895, acc = 0.52857 | Val: loss = 1.27007, acc = 0.79200\n",
      "Training: loss = 1.29164, acc = 0.64286 | Val: loss = 1.26794, acc = 0.79600\n",
      "Training: loss = 1.44833, acc = 0.55714 | Val: loss = 1.26928, acc = 0.79600\n",
      "Early stop! Min loss:  1.240984320640564 , Max accuracy:  0.8240000605583191\n",
      "Early stop model validation loss:  tf.Tensor(1.2516618, shape=(), dtype=float32) , accuracy:  tf.Tensor(0.816, shape=(), dtype=float32)\n",
      "Test loss: tf.Tensor(1.2209096, shape=(), dtype=float32) ; Test accuracy: tf.Tensor(0.839, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from models import GAT\n",
    "#from utils import process\n",
    "\n",
    "print('Dataset: ' + dataset)\n",
    "print('----- Opt. hyperparams -----')\n",
    "print('lr: ' + str(lr))\n",
    "print('l2_coef: ' + str(l2_coef))\n",
    "print('----- Archi. hyperparams -----')\n",
    "print('nb. layers: ' + str(len(hid_units)))\n",
    "print('nb. units per layer: ' + str(hid_units))\n",
    "print('nb. attention heads: ' + str(n_heads))\n",
    "print('residual: ' + str(residual))\n",
    "print('nonlinearity: ' + str(nonlinearity))\n",
    "\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(dataset)\n",
    "\n",
    "features, spars = preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_size = features.shape[1]\n",
    "nb_classes = y_train.shape[1]\n",
    "\n",
    "\n",
    "features = features[np.newaxis]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "train_mask = train_mask[np.newaxis]\n",
    "val_mask = val_mask[np.newaxis]\n",
    "test_mask = test_mask[np.newaxis]\n",
    "\n",
    "\n",
    "print(f'These are the parameters')\n",
    "print(f'batch_size: {batch_size}')\n",
    "print(f'nb_nodes: {nb_nodes}')\n",
    "print(f'ft_size: {ft_size}')\n",
    "print(f'nb_classes: {nb_classes}')\n",
    "\n",
    "\n",
    "if Sparse:\n",
    "    biases = preprocess_adj_bias(adj)\n",
    "    \n",
    "else:\n",
    "    adj = adj.todense()\n",
    "    adj = adj[np.newaxis]\n",
    "    biases = adj_to_bias(adj, [nb_nodes], nhood=1)\n",
    "    \n",
    "    \n",
    "model = GAT(hid_units,n_heads, nb_classes, nb_nodes,Sparse,ffd_drop = ffd_drop,attn_drop = attn_drop,activation = tf.nn.elu,residual=False)\n",
    "print('model: ' + str('SpGAT' if Sparse else 'GAT'))\n",
    "\n",
    "\n",
    "vlss_mn = np.inf\n",
    "vacc_mx = 0.0\n",
    "curr_step = 0\n",
    "\n",
    "train_loss_avg = 0\n",
    "train_acc_avg = 0\n",
    "val_loss_avg = 0\n",
    "val_acc_avg = 0\n",
    "\n",
    "model_number = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    ###Training Segment###\n",
    "    tr_step = 0\n",
    "    tr_size = features.shape[0]\n",
    "    while tr_step * batch_size < tr_size:\n",
    "        \n",
    "        if Sparse:\n",
    "            bbias = biases\n",
    "        else:\n",
    "            bbias = biases[tr_step*batch_size:(tr_step+1)*batch_size]\n",
    "            \n",
    "        _, acc_tr,loss_value_tr = train(model,\n",
    "                                        inputs=     features[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                                        bias_mat=     bbias,\n",
    "                                        lbl_in =     y_train[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                                        msk_in =train_mask[tr_step*batch_size:(tr_step+1)*batch_size],\n",
    "                                        training=True)\n",
    "        train_loss_avg += loss_value_tr\n",
    "        train_acc_avg += acc_tr\n",
    "        tr_step += 1\n",
    "        \n",
    "    ###Validation Segment###\n",
    "    vl_step = 0\n",
    "    vl_size = features.shape[0]\n",
    "    while vl_step * batch_size < vl_size:\n",
    "        \n",
    "        if Sparse:\n",
    "            bbias = biases\n",
    "        else:\n",
    "            bbias = biases[vl_step*batch_size:(vl_step+1)*batch_size]\n",
    "        \n",
    "        _, acc_vl,loss_value_vl = evaluate(model,\n",
    "                                            inputs=     features[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                                            bias_mat=     bbias,\n",
    "                                            lbl_in =     y_val[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                                            msk_in =val_mask[vl_step*batch_size:(vl_step+1)*batch_size],\n",
    "                                            training=False)\n",
    "        val_loss_avg += loss_value_vl\n",
    "        val_acc_avg += acc_vl\n",
    "        vl_step += 1\n",
    "        \n",
    "    print('Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "                                        (train_loss_avg/tr_step, train_acc_avg/tr_step,\n",
    "                                        val_loss_avg/vl_step, val_acc_avg/vl_step))\n",
    "    \n",
    "\n",
    "    ###Early Stopping Segment###\n",
    "    \n",
    "    if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n",
    "            if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n",
    "                    vacc_early_model = val_acc_avg/vl_step\n",
    "                    vlss_early_model = val_loss_avg/vl_step            \n",
    "                    working_weights = model.get_weights()\n",
    "            vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n",
    "            vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n",
    "            curr_step = 0\n",
    "    else:\n",
    "            curr_step += 1\n",
    "            if curr_step == patience:\n",
    "                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n",
    "                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n",
    "                    model.set_weights(working_weights)\n",
    "                    break\n",
    "\n",
    "    train_loss_avg = 0\n",
    "    train_acc_avg = 0\n",
    "    val_loss_avg = 0\n",
    "    val_acc_avg = 0\n",
    "\n",
    "###Testing Segment### Outside of the epochs\n",
    "\n",
    "ts_step = 0\n",
    "ts_size = features.shape[0]\n",
    "ts_loss = 0.0\n",
    "ts_acc = 0.0\n",
    "while ts_step * batch_size < ts_size:\n",
    "    \n",
    "    if Sparse:\n",
    "            bbias = biases\n",
    "    else:\n",
    "            bbias = biases[ts_step*batch_size:(ts_step+1)*batch_size]\n",
    "    \n",
    "    _, acc_ts,loss_value_ts = evaluate(model,\n",
    "                                        inputs=     features[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                                        bias_mat=     bbias,\n",
    "                                        lbl_in =     y_test[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                                        msk_in =test_mask[ts_step*batch_size:(ts_step+1)*batch_size],\n",
    "                                        training=False)\n",
    "    ts_loss += loss_value_ts\n",
    "    ts_acc += acc_ts\n",
    "    ts_step += 1\n",
    "\n",
    "print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n",
    "#print('Test loss: %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' %\n",
    "#                                    (train_loss_avg/tr_step, train_acc_avg/tr_step,\n",
    "#                                    val_loss_avg/vl_step, val_acc_avg/vl_step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
